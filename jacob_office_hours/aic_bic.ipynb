{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection \n",
    "\n",
    "- $r^2$\n",
    "- Adjusted $r^2$\n",
    "- AIC\n",
    "- BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Example\n",
    "\n",
    "Which model is better if number of parameters differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sympy as sy\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficient of Determination\n",
    "\n",
    "- Regression Sum of Squares $SSR = \\sum(\\hat{y_i} - \\bar{y})^2$\n",
    "\n",
    "- Error Sum of Squares $SSE = \\sum(y_i - \\hat{y_i})^2$\n",
    "\n",
    "- Total Sum of Squares $SSTO = \\sum(y_i - \\bar{y})^2$\n",
    "\n",
    "$$r^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "bdf = pd.DataFrame(X, columns=boston.feature_names)\n",
    "bdf['target'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the boston data\n",
    "bdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split, fit, score\n",
    "def lin_reg_fitter(X, y):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    lr = LinearRegression()\n",
    "\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    return lr.score(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists of scores\n",
    "cols = []\n",
    "scores = []\n",
    "for col in boston.feature_names:\n",
    "    cols.append(col)\n",
    "    score = lin_reg_fitter(bdf[cols], bdf['target'])\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'num_features': [i for i in range(len(boston.feature_names))], 'r2': scores})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted $r^2$\n",
    "\n",
    "Adjust the $SSE$ and $SSTO$ by degrees of freedom.\n",
    "\n",
    "$$r^2_a = 1 - \\frac{SSE/(n-p)}{SSTO/(n-1)} = 1 - \\frac{MSE}{MSTO}$$\n",
    "\n",
    "$$r^2_a = r^2 - ( \\frac{p-1}{n-p}) (1 - r^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic Information Criteria\n",
    "\n",
    "$$xIC = -2ln(L) + complexity ~term$$\n",
    "\n",
    "$L$ is maximized likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Akaike Information Criterion\n",
    "\n",
    "$$AIC = -2 ln(L) + 2p$$\n",
    "\n",
    "$$AIC = n \\ln(SSE_p/n) + 2p$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Information Criterion\n",
    "\n",
    "$$BIC = -2 ln(L) + p ln(n)$$\n",
    "\n",
    "$$BIC = n\\ln(SSE_p/n) + p \\ln(n)$$\n",
    "\n",
    "Penalizes more complex more ($n \\geq 8$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "\n",
    "$$r^2_a \\quad \\text{or}\\quad  AIC \\quad \\text{or} \\quad BIC$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = sm.OLS(boston.target, bdf['CHAS'] + bdf['INDUS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lr.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr.fit().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.rsquared_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.bic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = MaxAbsScaler()\n",
    "X = ss.fit_transform(X)\n",
    "y = ss.fit_transform(y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.DataFrame(X, columns = iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.Logit(y, iris_df['sepal length (cm)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit().summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.Logit( y, iris_df['sepal length (cm)'] + iris_df['petal length (cm)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit().summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stats.stackexchange.com/questions/90769/using-bic-to-estimate-the-number-of-k-in-kmeans\n",
    "\n",
    "from sklearn import cluster\n",
    "from scipy.spatial import distance\n",
    "import sklearn.datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def compute_bic(kmeans,X):\n",
    "    \"\"\"\n",
    "    Computes the BIC metric for a given clusters\n",
    "\n",
    "    Parameters:\n",
    "    -----------------------------------------\n",
    "    kmeans:  List of clustering object from scikit learn\n",
    "\n",
    "    X     :  multidimension np array of data points\n",
    "\n",
    "    Returns:\n",
    "    -----------------------------------------\n",
    "    BIC value\n",
    "    \"\"\"\n",
    "    # assign centers and labels\n",
    "    centers = [kmeans.cluster_centers_]\n",
    "    labels  = kmeans.labels_\n",
    "    #number of clusters\n",
    "    m = kmeans.n_clusters\n",
    "    # size of the clusters\n",
    "    n = np.bincount(labels)\n",
    "    #size of data set\n",
    "    N, d = X.shape\n",
    "\n",
    "    #compute variance for all clusters beforehand\n",
    "    cl_var = (1.0 / (N - m) / d) * sum([sum(distance.cdist(X[np.where(labels == i)], [centers[0][i]], \n",
    "             'euclidean')**2) for i in range(m)])\n",
    "\n",
    "    const_term = 0.5 * m * np.log(N) * (d+1)\n",
    "\n",
    "    BIC = np.sum([n[i] * np.log(n[i]) -\n",
    "               n[i] * np.log(N) -\n",
    "             ((n[i] * d) / 2) * np.log(2*np.pi*cl_var) -\n",
    "             ((n[i] - 1) * d/ 2) for i in range(m)]) - const_term\n",
    "\n",
    "    return(BIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IRIS DATA\n",
    "iris = sklearn.datasets.load_iris()\n",
    "X = iris.data[:, :4]  # extract only the features\n",
    "#Xs = StandardScaler().fit_transform(X)\n",
    "Y = iris.target\n",
    "\n",
    "ks = range(1,10)\n",
    "\n",
    "# run 9 times kmeans and save each result in the KMeans object\n",
    "KMeans = [cluster.KMeans(n_clusters = i, init=\"k-means++\").fit(X) for i in ks]\n",
    "\n",
    "# now run for each cluster the BIC computation\n",
    "BIC = [compute_bic(kmeansi,X) for kmeansi in KMeans]\n",
    "\n",
    "plt.bar(ks, BIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn import mixture\n",
    "# Number of samples per component\n",
    "n_samples = 500\n",
    "\n",
    "# Generate random sample, two components\n",
    "np.random.seed(0)\n",
    "C = np.array([[0., -0.1], [1.7, .4]])\n",
    "X = np.r_[np.dot(np.random.randn(n_samples, 2), C),\n",
    "          .7 * np.random.randn(n_samples, 2) + np.array([-6, 3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_bic = np.infty\n",
    "bic = []\n",
    "n_components_range = range(1, 7)\n",
    "cv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "for cv_type in cv_types:\n",
    "    for n_components in n_components_range:\n",
    "        # Fit a Gaussian mixture with EM\n",
    "        gmm = mixture.GaussianMixture(n_components=n_components,\n",
    "                                      covariance_type=cv_type)\n",
    "        gmm.fit(X)\n",
    "        bic.append(gmm.bic(X))\n",
    "        if bic[-1] < lowest_bic:\n",
    "            lowest_bic = bic[-1]\n",
    "            best_gmm = gmm\n",
    "\n",
    "bic = np.array(bic)\n",
    "color_iter = itertools.cycle(['navy', 'turquoise', 'cornflowerblue',\n",
    "                              'darkorange'])\n",
    "clf = best_gmm\n",
    "bars = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the BIC scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "spl = plt.subplot(2, 1, 1)\n",
    "for i, (cv_type, color) in enumerate(zip(cv_types, color_iter)):\n",
    "    xpos = np.array(n_components_range) + .2 * (i - 2)\n",
    "    bars.append(plt.bar(xpos, bic[i * len(n_components_range):\n",
    "                                  (i + 1) * len(n_components_range)],\n",
    "                        width=.2, color=color))\n",
    "plt.xticks(n_components_range)\n",
    "plt.ylim([bic.min() * 1.01 - .01 * bic.max(), bic.max()])\n",
    "plt.title('BIC score per model')\n",
    "xpos = np.mod(bic.argmin(), len(n_components_range)) + .65 +\\\n",
    "    .2 * np.floor(bic.argmin() / len(n_components_range))\n",
    "plt.text(xpos, bic.min() * 0.97 + .03 * bic.max(), '*', fontsize=14)\n",
    "spl.set_xlabel('Number of components')\n",
    "spl.legend([b[0] for b in bars], cv_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the winner\n",
    "splot = plt.subplot(2, 1, 2)\n",
    "Y_ = clf.predict(X)\n",
    "for i, (mean, cov, color) in enumerate(zip(clf.means_, clf.covariances_,\n",
    "                                           color_iter)):\n",
    "    v, w = linalg.eigh(cov)\n",
    "    if not np.any(Y_ == i):\n",
    "        continue\n",
    "    plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)\n",
    "\n",
    "    # Plot an ellipse to show the Gaussian component\n",
    "    angle = np.arctan2(w[0][1], w[0][0])\n",
    "    angle = 180. * angle / np.pi  # convert to degrees\n",
    "    v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "    ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n",
    "    ell.set_clip_box(splot.bbox)\n",
    "    ell.set_alpha(.5)\n",
    "    splot.add_artist(ell)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title('Selected GMM: full model, 2 components')\n",
    "plt.subplots_adjust(hspace=.35, bottom=.02)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
